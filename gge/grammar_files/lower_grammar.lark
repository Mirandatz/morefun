start: backbone optimizer

optimizer: sgd | adam

sgd           : _SGD learning_rate momentum nesterov
learning_rate : _LEARNING_RATE FLOAT
momentum      : _MOMENTUM FLOAT
nesterov      : _NESTEROV BOOL

adam    : _ADAM learning_rate beta1 beta2 epsilon amsgrad
beta1   : _BETA1 FLOAT
beta2   : _BETA2 FLOAT
epsilon : _EPSILON FLOAT
amsgrad : _AMSGRAD BOOL

backbone : block+
block    : MERGE? layer FORK?
layer    : conv
         | max_pool
         | avg_pool
         | batchnorm
         | activation
    
conv         : _CONV filter_count kernel_size stride
filter_count : _FILTER_COUNT INT
kernel_size  : _KERNEL_SIZE INT
stride       : _STRIDE INT

max_pool  : _MAX_POOL pool_size stride
avg_pool  : _AVG_POOL pool_size stride
pool_size : _POOL_SIZE INT
 
batchnorm : _BATCHNORM

activation : RELU | GELU | SWISH

%import .terminals (MERGE, FORK, BOOL, RELU, GELU, SWISH)

%import .terminals.CONV         -> _CONV
%import .terminals.FILTER_COUNT -> _FILTER_COUNT 
%import .terminals.KERNEL_SIZE  -> _KERNEL_SIZE
%import .terminals.STRIDE       -> _STRIDE

%import .terminals.MAX_POOL  -> _MAX_POOL
%import .terminals.AVG_POOL  -> _AVG_POOL
%import .terminals.POOL_SIZE -> _POOL_SIZE

%import .terminals.BATCHNORM -> _BATCHNORM

%import .terminals.SGD           -> _SGD
%import .terminals.LEARNING_RATE -> _LEARNING_RATE
%import .terminals.MOMENTUM      -> _MOMENTUM
%import .terminals.NESTEROV      -> _NESTEROV

%import .terminals.ADAM    -> _ADAM
%import .terminals.BETA1   -> _BETA1
%import .terminals.BETA2   -> _BETA2
%import .terminals.EPSILON -> _EPSILON
%import .terminals.AMSGRAD -> _AMSGRAD

%import common (INT, FLOAT)
%import common.WS

%ignore WS