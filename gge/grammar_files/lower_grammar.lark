start: backbone optimizer

optimizer: sgd | adam

sgd           : SGD learning_rate momentum nesterov
learning_rate : LEARNING_RATE FLOAT
momentum      : MOMENTUM FLOAT
nesterov      : NESTEROV BOOL

adam    : _ADAM learning_rate beta1 beta2 epsilon amsgrad
beta1   : _BETA1 FLOAT
beta2   : _BETA2 FLOAT
epsilon : _EPSILON FLOAT
amsgrad : _AMSGRAD BOOL

backbone : block+
block    : MERGE? layer FORK?
layer    : conv
         | maxpool
         | avgpool
         | batchnorm
         | activation
    
conv         : CONV filter_count kernel_size stride
filter_count : FILTER_COUNT INT
kernel_size  : KERNEL_SIZE INT
stride       : STRIDE INT

maxpool  : MAXPOOL pool_size stride
avgpool  : AVGPOOL pool_size stride
pool_size : POOL_SIZE INT
 
batchnorm : _BATCHNORM

activation : RELU | GELU | SWISH

%import .terminals (MERGE, FORK, BOOL, RELU, GELU, SWISH)

%import .terminals (CONV, FILTER_COUNT, KERNEL_SIZE, STRIDE)
%import .terminals (MAXPOOL, AVGPOOL, POOL_SIZE)

%import .terminals.BATCHNORM -> _BATCHNORM

%import .terminals (SGD, LEARNING_RATE, MOMENTUM, NESTEROV)

%import .terminals.ADAM    -> _ADAM
%import .terminals.BETA1   -> _BETA1
%import .terminals.BETA2   -> _BETA2
%import .terminals.EPSILON -> _EPSILON
%import .terminals.AMSGRAD -> _AMSGRAD

%import common (INT, FLOAT)
%import common.WS

%ignore WS
